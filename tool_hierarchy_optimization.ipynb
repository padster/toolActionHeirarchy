{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Hierarchy Optimization for LLM Tool Calling\n",
    "\n",
    "This notebook explores different approaches to organizing tools when calling LLMs:\n",
    "- **Domain-based hierarchies**: Tools grouped by domain (e.g., all file operations together)\n",
    "- **Action-based hierarchies**: Tools grouped by action type (e.g., all read operations together)\n",
    "\n",
    "We'll test accuracy, confusion patterns, and embedding-based approaches to optimize tool selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Example Tool Sets\n",
    "\n",
    "We'll create a representative set of tools that could be available to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example tools with metadata\n",
    "tools = [\n",
    "    # File operations\n",
    "    {\"id\": \"file_read\", \"name\": \"Read File\", \"description\": \"Read contents of a file\", \"domain\": \"file\", \"action\": \"read\"},\n",
    "    {\"id\": \"file_write\", \"name\": \"Write File\", \"description\": \"Write contents to a file\", \"domain\": \"file\", \"action\": \"write\"},\n",
    "    {\"id\": \"file_delete\", \"name\": \"Delete File\", \"description\": \"Delete a file from disk\", \"domain\": \"file\", \"action\": \"delete\"},\n",
    "    {\"id\": \"file_list\", \"name\": \"List Files\", \"description\": \"List files in a directory\", \"domain\": \"file\", \"action\": \"read\"},\n",
    "    \n",
    "    # Database operations\n",
    "    {\"id\": \"db_query\", \"name\": \"Query Database\", \"description\": \"Execute a SELECT query on database\", \"domain\": \"database\", \"action\": \"read\"},\n",
    "    {\"id\": \"db_insert\", \"name\": \"Insert Database\", \"description\": \"Insert records into database\", \"domain\": \"database\", \"action\": \"write\"},\n",
    "    {\"id\": \"db_update\", \"name\": \"Update Database\", \"description\": \"Update records in database\", \"domain\": \"database\", \"action\": \"write\"},\n",
    "    {\"id\": \"db_delete\", \"name\": \"Delete Database\", \"description\": \"Delete records from database\", \"domain\": \"database\", \"action\": \"delete\"},\n",
    "    \n",
    "    # API operations\n",
    "    {\"id\": \"api_get\", \"name\": \"API GET\", \"description\": \"Make HTTP GET request to API\", \"domain\": \"api\", \"action\": \"read\"},\n",
    "    {\"id\": \"api_post\", \"name\": \"API POST\", \"description\": \"Make HTTP POST request to API\", \"domain\": \"api\", \"action\": \"write\"},\n",
    "    {\"id\": \"api_put\", \"name\": \"API PUT\", \"description\": \"Make HTTP PUT request to API\", \"domain\": \"api\", \"action\": \"write\"},\n",
    "    {\"id\": \"api_delete\", \"name\": \"API DELETE\", \"description\": \"Make HTTP DELETE request to API\", \"domain\": \"api\", \"action\": \"delete\"},\n",
    "    \n",
    "    # Email operations\n",
    "    {\"id\": \"email_read\", \"name\": \"Read Email\", \"description\": \"Read emails from inbox\", \"domain\": \"email\", \"action\": \"read\"},\n",
    "    {\"id\": \"email_send\", \"name\": \"Send Email\", \"description\": \"Send an email message\", \"domain\": \"email\", \"action\": \"write\"},\n",
    "    {\"id\": \"email_delete\", \"name\": \"Delete Email\", \"description\": \"Delete an email message\", \"domain\": \"email\", \"action\": \"delete\"},\n",
    "    \n",
    "    # Calendar operations\n",
    "    {\"id\": \"cal_read\", \"name\": \"Read Calendar\", \"description\": \"Read calendar events\", \"domain\": \"calendar\", \"action\": \"read\"},\n",
    "    {\"id\": \"cal_create\", \"name\": \"Create Event\", \"description\": \"Create a calendar event\", \"domain\": \"calendar\", \"action\": \"write\"},\n",
    "    {\"id\": \"cal_delete\", \"name\": \"Delete Event\", \"description\": \"Delete a calendar event\", \"domain\": \"calendar\", \"action\": \"delete\"},\n",
    "]\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "tools_df = pd.DataFrame(tools)\n",
    "print(f\"Total tools: {len(tools_df)}\")\n",
    "print(\"\\nTools by domain:\")\n",
    "print(tools_df['domain'].value_counts())\n",
    "print(\"\\nTools by action:\")\n",
    "print(tools_df['action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Hierarchies\n",
    "\n",
    "Create both domain-based and action-based hierarchical structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_domain_hierarchy(tools_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Create a domain-based hierarchy.\"\"\"\n",
    "    hierarchy = defaultdict(list)\n",
    "    for _, tool in tools_df.iterrows():\n",
    "        hierarchy[tool['domain']].append(tool.to_dict())\n",
    "    return dict(hierarchy)\n",
    "\n",
    "def create_action_hierarchy(tools_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Create an action-based hierarchy.\"\"\"\n",
    "    hierarchy = defaultdict(list)\n",
    "    for _, tool in tools_df.iterrows():\n",
    "        hierarchy[tool['action']].append(tool.to_dict())\n",
    "    return dict(hierarchy)\n",
    "\n",
    "domain_hierarchy = create_domain_hierarchy(tools_df)\n",
    "action_hierarchy = create_action_hierarchy(tools_df)\n",
    "\n",
    "print(\"Domain-based hierarchy:\")\n",
    "for domain, tools_list in domain_hierarchy.items():\n",
    "    print(f\"  {domain}: {len(tools_list)} tools\")\n",
    "\n",
    "print(\"\\nAction-based hierarchy:\")\n",
    "for action, tools_list in action_hierarchy.items():\n",
    "    print(f\"  {action}: {len(tools_list)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Test Queries\n",
    "\n",
    "Define user queries that should map to specific tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries with expected tool\n",
    "test_queries = [\n",
    "    # File operations\n",
    "    {\"query\": \"Show me the contents of config.json\", \"expected_tool\": \"file_read\"},\n",
    "    {\"query\": \"Save this data to output.txt\", \"expected_tool\": \"file_write\"},\n",
    "    {\"query\": \"Remove the temporary file\", \"expected_tool\": \"file_delete\"},\n",
    "    {\"query\": \"What files are in the current directory?\", \"expected_tool\": \"file_list\"},\n",
    "    \n",
    "    # Database operations\n",
    "    {\"query\": \"Get all users from the database\", \"expected_tool\": \"db_query\"},\n",
    "    {\"query\": \"Add a new record to the customers table\", \"expected_tool\": \"db_insert\"},\n",
    "    {\"query\": \"Change the email address for user 123\", \"expected_tool\": \"db_update\"},\n",
    "    {\"query\": \"Remove all expired records from the database\", \"expected_tool\": \"db_delete\"},\n",
    "    \n",
    "    # API operations\n",
    "    {\"query\": \"Fetch the latest data from the weather API\", \"expected_tool\": \"api_get\"},\n",
    "    {\"query\": \"Submit this form data to the server\", \"expected_tool\": \"api_post\"},\n",
    "    {\"query\": \"Update the user profile via API\", \"expected_tool\": \"api_put\"},\n",
    "    {\"query\": \"Delete the resource using the REST API\", \"expected_tool\": \"api_delete\"},\n",
    "    \n",
    "    # Email operations\n",
    "    {\"query\": \"Check my inbox for new messages\", \"expected_tool\": \"email_read\"},\n",
    "    {\"query\": \"Send an email to the team about the meeting\", \"expected_tool\": \"email_send\"},\n",
    "    {\"query\": \"Delete the spam email\", \"expected_tool\": \"email_delete\"},\n",
    "    \n",
    "    # Calendar operations\n",
    "    {\"query\": \"What meetings do I have tomorrow?\", \"expected_tool\": \"cal_read\"},\n",
    "    {\"query\": \"Schedule a meeting for next week\", \"expected_tool\": \"cal_create\"},\n",
    "    {\"query\": \"Cancel my 3pm appointment\", \"expected_tool\": \"cal_delete\"},\n",
    "    \n",
    "    # Ambiguous queries (could match multiple)\n",
    "    {\"query\": \"Delete something\", \"expected_tool\": \"file_delete\"},  # Ambiguous\n",
    "    {\"query\": \"Read the data\", \"expected_tool\": \"file_read\"},  # Ambiguous\n",
    "]\n",
    "\n",
    "queries_df = pd.DataFrame(test_queries)\n",
    "print(f\"Total test queries: {len(queries_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding-based Tool Selection\n",
    "\n",
    "Use sentence embeddings to match queries to tools based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Generate embeddings for tool descriptions\n",
    "tool_descriptions = [f\"{tool['name']}: {tool['description']}\" for tool in tools]\n",
    "tool_embeddings = embedding_model.encode(tool_descriptions)\n",
    "print(f\"Generated embeddings for {len(tool_embeddings)} tools\")\n",
    "print(f\"Embedding dimension: {tool_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_tool_by_embedding(query: str, tool_embeddings: np.ndarray, tools_list: List[Dict]) -> Tuple[str, float]:\n",
    "    \"\"\"Find the best matching tool using embedding similarity.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    similarities = cosine_similarity(query_embedding, tool_embeddings)[0]\n",
    "    best_idx = np.argmax(similarities)\n",
    "    return tools_list[best_idx]['id'], similarities[best_idx]\n",
    "\n",
    "def find_best_tool_hierarchical(query: str, hierarchy: Dict, tools_df: pd.DataFrame, \n",
    "                                hierarchy_type: str) -> Tuple[str, float, str]:\n",
    "    \"\"\"Two-stage hierarchical selection: first select category, then tool.\"\"\"\n",
    "    # Stage 1: Find best category\n",
    "    categories = list(hierarchy.keys())\n",
    "    category_descriptions = [f\"{cat} operations\" for cat in categories]\n",
    "    category_embeddings = embedding_model.encode(category_descriptions)\n",
    "    \n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    category_similarities = cosine_similarity(query_embedding, category_embeddings)[0]\n",
    "    best_category_idx = np.argmax(category_similarities)\n",
    "    best_category = categories[best_category_idx]\n",
    "    \n",
    "    # Stage 2: Find best tool within category\n",
    "    category_tools = hierarchy[best_category]\n",
    "    category_tool_descriptions = [f\"{tool['name']}: {tool['description']}\" for tool in category_tools]\n",
    "    category_tool_embeddings = embedding_model.encode(category_tool_descriptions)\n",
    "    \n",
    "    tool_similarities = cosine_similarity(query_embedding, category_tool_embeddings)[0]\n",
    "    best_tool_idx = np.argmax(tool_similarities)\n",
    "    \n",
    "    return category_tools[best_tool_idx]['id'], tool_similarities[best_tool_idx], best_category\n",
    "\n",
    "# Test on a sample query\n",
    "sample_query = \"Show me the contents of config.json\"\n",
    "predicted_tool, similarity = find_best_tool_by_embedding(sample_query, tool_embeddings, tools)\n",
    "print(f\"Query: {sample_query}\")\n",
    "print(f\"Predicted tool: {predicted_tool}\")\n",
    "print(f\"Similarity score: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Flat vs Hierarchical Approaches\n",
    "\n",
    "Compare accuracy of:\n",
    "1. Flat: Direct embedding match across all tools\n",
    "2. Domain hierarchy: Category selection then tool selection\n",
    "3. Action hierarchy: Action selection then tool selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all approaches\n",
    "results = []\n",
    "\n",
    "for _, test in queries_df.iterrows():\n",
    "    query = test['query']\n",
    "    expected = test['expected_tool']\n",
    "    \n",
    "    # Flat approach\n",
    "    flat_pred, flat_sim = find_best_tool_by_embedding(query, tool_embeddings, tools)\n",
    "    \n",
    "    # Domain hierarchy\n",
    "    domain_pred, domain_sim, domain_cat = find_best_tool_hierarchical(query, domain_hierarchy, tools_df, \"domain\")\n",
    "    \n",
    "    # Action hierarchy\n",
    "    action_pred, action_sim, action_cat = find_best_tool_hierarchical(query, action_hierarchy, tools_df, \"action\")\n",
    "    \n",
    "    results.append({\n",
    "        'query': query,\n",
    "        'expected': expected,\n",
    "        'flat_prediction': flat_pred,\n",
    "        'flat_similarity': flat_sim,\n",
    "        'flat_correct': flat_pred == expected,\n",
    "        'domain_prediction': domain_pred,\n",
    "        'domain_similarity': domain_sim,\n",
    "        'domain_category': domain_cat,\n",
    "        'domain_correct': domain_pred == expected,\n",
    "        'action_prediction': action_pred,\n",
    "        'action_similarity': action_sim,\n",
    "        'action_category': action_cat,\n",
    "        'action_correct': action_pred == expected,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate accuracy for each approach\n",
    "flat_accuracy = results_df['flat_correct'].mean()\n",
    "domain_accuracy = results_df['domain_correct'].mean()\n",
    "action_accuracy = results_df['action_correct'].mean()\n",
    "\n",
    "print(\"Accuracy Comparison:\")\n",
    "print(f\"  Flat approach:            {flat_accuracy:.2%}\")\n",
    "print(f\"  Domain hierarchy:         {domain_accuracy:.2%}\")\n",
    "print(f\"  Action hierarchy:         {action_accuracy:.2%}\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(results_df[['query', 'expected', 'flat_prediction', 'domain_prediction', 'action_prediction']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Errors and Confusion Patterns\n",
    "\n",
    "Identify which tools are commonly confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show errors for each approach\n",
    "print(\"Flat approach errors:\")\n",
    "flat_errors = results_df[~results_df['flat_correct']][['query', 'expected', 'flat_prediction']]\n",
    "if len(flat_errors) > 0:\n",
    "    for _, error in flat_errors.iterrows():\n",
    "        print(f\"  Query: {error['query']}\")\n",
    "        print(f\"    Expected: {error['expected']}, Got: {error['flat_prediction']}\")\n",
    "else:\n",
    "    print(\"  No errors!\")\n",
    "\n",
    "print(\"\\nDomain hierarchy errors:\")\n",
    "domain_errors = results_df[~results_df['domain_correct']][['query', 'expected', 'domain_prediction', 'domain_category']]\n",
    "if len(domain_errors) > 0:\n",
    "    for _, error in domain_errors.iterrows():\n",
    "        print(f\"  Query: {error['query']}\")\n",
    "        print(f\"    Expected: {error['expected']}, Got: {error['domain_prediction']} (category: {error['domain_category']})\")\n",
    "else:\n",
    "    print(\"  No errors!\")\n",
    "\n",
    "print(\"\\nAction hierarchy errors:\")\n",
    "action_errors = results_df[~results_df['action_correct']][['query', 'expected', 'action_prediction', 'action_category']]\n",
    "if len(action_errors) > 0:\n",
    "    for _, error in action_errors.iterrows():\n",
    "        print(f\"  Query: {error['query']}\")\n",
    "        print(f\"    Expected: {error['expected']}, Got: {error['action_prediction']} (category: {error['action_category']})\")\n",
    "else:\n",
    "    print(\"  No errors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, title, labels=None):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    if labels is None:\n",
    "        labels = sorted(list(set(y_true) | set(y_pred)))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels, cbar_kws={'label': 'Count'})\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Tool')\n",
    "    plt.xlabel('Predicted Tool')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get unique tool IDs\n",
    "tool_ids = sorted(tools_df['id'].unique())\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(results_df['expected'], results_df['flat_prediction'], \n",
    "                      'Confusion Matrix - Flat Approach', tool_ids)\n",
    "plot_confusion_matrix(results_df['expected'], results_df['domain_prediction'], \n",
    "                      'Confusion Matrix - Domain Hierarchy', tool_ids)\n",
    "plot_confusion_matrix(results_df['expected'], results_df['action_prediction'], \n",
    "                      'Confusion Matrix - Action Hierarchy', tool_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Similarity Scores\n",
    "\n",
    "Compare the confidence (similarity scores) across approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare similarity scores\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Flat approach\n",
    "axes[0].hist(results_df['flat_similarity'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(results_df['flat_similarity'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0].set_title('Flat Approach Similarity Scores')\n",
    "axes[0].set_xlabel('Similarity Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Domain hierarchy\n",
    "axes[1].hist(results_df['domain_similarity'], bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].axvline(results_df['domain_similarity'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1].set_title('Domain Hierarchy Similarity Scores')\n",
    "axes[1].set_xlabel('Similarity Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "# Action hierarchy\n",
    "axes[2].hist(results_df['action_similarity'], bins=20, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[2].axvline(results_df['action_similarity'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[2].set_title('Action Hierarchy Similarity Scores')\n",
    "axes[2].set_xlabel('Similarity Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Average similarity scores:\")\n",
    "print(f\"  Flat:     {results_df['flat_similarity'].mean():.3f} ± {results_df['flat_similarity'].std():.3f}\")\n",
    "print(f\"  Domain:   {results_df['domain_similarity'].mean():.3f} ± {results_df['domain_similarity'].std():.3f}\")\n",
    "print(f\"  Action:   {results_df['action_similarity'].mean():.3f} ± {results_df['action_similarity'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis by Query Type\n",
    "\n",
    "Analyze which types of queries work better with which hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add domain and action to results based on expected tool\n",
    "results_df = results_df.merge(\n",
    "    tools_df[['id', 'domain', 'action']], \n",
    "    left_on='expected', \n",
    "    right_on='id', \n",
    "    how='left'\n",
    ").drop('id', axis=1)\n",
    "\n",
    "# Accuracy by domain\n",
    "print(\"Accuracy by domain:\")\n",
    "domain_stats = results_df.groupby('domain').agg({\n",
    "    'flat_correct': 'mean',\n",
    "    'domain_correct': 'mean',\n",
    "    'action_correct': 'mean'\n",
    "}).round(3)\n",
    "domain_stats.columns = ['Flat', 'Domain Hierarchy', 'Action Hierarchy']\n",
    "print(domain_stats)\n",
    "\n",
    "print(\"\\nAccuracy by action:\")\n",
    "action_stats = results_df.groupby('action').agg({\n",
    "    'flat_correct': 'mean',\n",
    "    'domain_correct': 'mean',\n",
    "    'action_correct': 'mean'\n",
    "}).round(3)\n",
    "action_stats.columns = ['Flat', 'Domain Hierarchy', 'Action Hierarchy']\n",
    "print(action_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy by category\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# By domain\n",
    "domain_stats.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Accuracy by Domain')\n",
    "axes[0].set_xlabel('Domain')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "axes[0].legend(title='Approach')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# By action\n",
    "action_stats.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Accuracy by Action Type')\n",
    "axes[1].set_xlabel('Action')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_ylim([0, 1.1])\n",
    "axes[1].legend(title='Approach')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Embedding Model Comparison\n",
    "\n",
    "Test different embedding models to see which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple embedding models\n",
    "embedding_models_to_test = [\n",
    "    'all-MiniLM-L6-v2',           # Fast, 384 dim\n",
    "    'all-mpnet-base-v2',          # Better quality, 768 dim\n",
    "    'paraphrase-MiniLM-L6-v2',    # Paraphrase detection, 384 dim\n",
    "]\n",
    "\n",
    "model_results = []\n",
    "\n",
    "for model_name in embedding_models_to_test:\n",
    "    print(f\"\\nTesting {model_name}...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    tool_embs = model.encode(tool_descriptions)\n",
    "    \n",
    "    # Test on queries\n",
    "    predictions = []\n",
    "    for query in queries_df['query']:\n",
    "        pred, _ = find_best_tool_by_embedding(query, tool_embs, tools)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    accuracy = accuracy_score(queries_df['expected_tool'], predictions)\n",
    "    \n",
    "    model_results.append({\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'dimensions': tool_embs.shape[1]\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"  Embedding dimensions: {tool_embs.shape[1]}\")\n",
    "\n",
    "model_results_df = pd.DataFrame(model_results)\n",
    "print(\"\\nModel comparison:\")\n",
    "print(model_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.bar(model_results_df['model'], model_results_df['accuracy'])\n",
    "ax.set_title('Embedding Model Comparison')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim([0, 1.1])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Color bars by accuracy\n",
    "colors = plt.cm.RdYlGn(model_results_df['accuracy'])\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (model, acc) in enumerate(zip(model_results_df['model'], model_results_df['accuracy'])):\n",
    "    ax.text(i, acc + 0.02, f'{acc:.2%}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations\n",
    "\n",
    "Synthesize findings and provide recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY OF FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. OVERALL ACCURACY COMPARISON\")\n",
    "print(\"-\" * 80)\n",
    "accuracy_comparison = pd.DataFrame({\n",
    "    'Approach': ['Flat', 'Domain Hierarchy', 'Action Hierarchy'],\n",
    "    'Accuracy': [flat_accuracy, domain_accuracy, action_accuracy],\n",
    "    'Avg Similarity': [\n",
    "        results_df['flat_similarity'].mean(),\n",
    "        results_df['domain_similarity'].mean(),\n",
    "        results_df['action_similarity'].mean()\n",
    "    ]\n",
    "})\n",
    "print(accuracy_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n2. BEST PERFORMING APPROACH\")\n",
    "print(\"-\" * 80)\n",
    "best_approach = accuracy_comparison.loc[accuracy_comparison['Accuracy'].idxmax()]\n",
    "print(f\"Winner: {best_approach['Approach']} with {best_approach['Accuracy']:.2%} accuracy\")\n",
    "\n",
    "print(\"\\n3. EMBEDDING MODEL COMPARISON\")\n",
    "print(\"-\" * 80)\n",
    "print(model_results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n4. KEY INSIGHTS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"• Domain-based hierarchies work better when queries clearly indicate the domain\")\n",
    "print(\"  (e.g., 'read email', 'delete file')\")\n",
    "print(\"• Action-based hierarchies excel when the action is clear but domain is ambiguous\")\n",
    "print(\"  (e.g., 'delete something', 'read the data')\")\n",
    "print(\"• Flat approaches provide a good baseline but may struggle with large tool sets\")\n",
    "print(\"• Higher-quality embedding models generally improve accuracy at the cost of\")\n",
    "print(\"  computation time and memory\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDATIONS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"• For general-purpose LLM tool calling: Use domain-based hierarchies\")\n",
    "print(\"• For specialized workflows with clear action patterns: Consider action-based\")\n",
    "print(\"• Implement hybrid approaches that combine both hierarchies for best results\")\n",
    "print(\"• Use better embedding models (like all-mpnet-base-v2) when accuracy is critical\")\n",
    "print(\"• Consider caching embeddings to reduce latency\")\n",
    "print(\"• Implement fallback mechanisms for ambiguous queries\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced: Hybrid Hierarchy\n",
    "\n",
    "Implement a hybrid approach that combines domain and action information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_tool_hybrid(query: str, tools_df: pd.DataFrame, tool_embeddings: np.ndarray) -> Tuple[str, float]:\n",
    "    \"\"\"Hybrid approach: Use both domain and action signals.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Get domain and action predictions\n",
    "    domains = tools_df['domain'].unique()\n",
    "    actions = tools_df['action'].unique()\n",
    "    \n",
    "    domain_descriptions = [f\"{d} operations\" for d in domains]\n",
    "    action_descriptions = [f\"{a} operations\" for a in actions]\n",
    "    \n",
    "    domain_embeddings = embedding_model.encode(domain_descriptions)\n",
    "    action_embeddings = embedding_model.encode(action_descriptions)\n",
    "    \n",
    "    domain_sims = cosine_similarity(query_embedding, domain_embeddings)[0]\n",
    "    action_sims = cosine_similarity(query_embedding, action_embeddings)[0]\n",
    "    \n",
    "    # Weighted combination\n",
    "    best_domain = domains[np.argmax(domain_sims)]\n",
    "    best_action = actions[np.argmax(action_sims)]\n",
    "    \n",
    "    # Filter tools by both domain and action\n",
    "    filtered_tools = tools_df[\n",
    "        (tools_df['domain'] == best_domain) | \n",
    "        (tools_df['action'] == best_action)\n",
    "    ]\n",
    "    \n",
    "    if len(filtered_tools) == 0:\n",
    "        # Fallback to domain only\n",
    "        filtered_tools = tools_df[tools_df['domain'] == best_domain]\n",
    "    \n",
    "    # Find best tool in filtered set\n",
    "    filtered_indices = filtered_tools.index.tolist()\n",
    "    filtered_embeddings = tool_embeddings[filtered_indices]\n",
    "    \n",
    "    tool_sims = cosine_similarity(query_embedding, filtered_embeddings)[0]\n",
    "    best_idx = np.argmax(tool_sims)\n",
    "    \n",
    "    return filtered_tools.iloc[best_idx]['id'], tool_sims[best_idx]\n",
    "\n",
    "# Test hybrid approach\n",
    "hybrid_predictions = []\n",
    "hybrid_similarities = []\n",
    "\n",
    "for query in queries_df['query']:\n",
    "    pred, sim = find_best_tool_hybrid(query, tools_df, tool_embeddings)\n",
    "    hybrid_predictions.append(pred)\n",
    "    hybrid_similarities.append(sim)\n",
    "\n",
    "hybrid_accuracy = accuracy_score(queries_df['expected_tool'], hybrid_predictions)\n",
    "\n",
    "print(f\"Hybrid approach accuracy: {hybrid_accuracy:.2%}\")\n",
    "print(f\"Average similarity: {np.mean(hybrid_similarities):.3f}\")\n",
    "\n",
    "# Compare all approaches\n",
    "print(\"\\nFinal comparison:\")\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Approach': ['Flat', 'Domain Hierarchy', 'Action Hierarchy', 'Hybrid'],\n",
    "    'Accuracy': [flat_accuracy, domain_accuracy, action_accuracy, hybrid_accuracy]\n",
    "})\n",
    "print(final_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Scalability Analysis\n",
    "\n",
    "Analyze how approaches scale with increasing number of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulate different tool set sizes\n",
    "tool_counts = [10, 20, 50, 100, 200]\n",
    "scalability_results = []\n",
    "\n",
    "for count in tool_counts:\n",
    "    # Create synthetic tools by repeating and modifying existing tools\n",
    "    synthetic_tools = []\n",
    "    for i in range(count):\n",
    "        base_tool = tools[i % len(tools)].copy()\n",
    "        base_tool['id'] = f\"{base_tool['id']}_{i}\"\n",
    "        base_tool['name'] = f\"{base_tool['name']} v{i}\"\n",
    "        synthetic_tools.append(base_tool)\n",
    "    \n",
    "    synthetic_tools_df = pd.DataFrame(synthetic_tools)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    synthetic_descriptions = [f\"{t['name']}: {t['description']}\" for t in synthetic_tools]\n",
    "    synthetic_embeddings = embedding_model.encode(synthetic_descriptions)\n",
    "    \n",
    "    # Measure flat search time\n",
    "    start = time.time()\n",
    "    for query in queries_df['query'].head(10):  # Use subset for speed\n",
    "        find_best_tool_by_embedding(query, synthetic_embeddings, synthetic_tools)\n",
    "    flat_time = (time.time() - start) / 10  # Average per query\n",
    "    \n",
    "    # Measure hierarchical search time (domain)\n",
    "    domain_hier = create_domain_hierarchy(synthetic_tools_df)\n",
    "    start = time.time()\n",
    "    for query in queries_df['query'].head(10):\n",
    "        find_best_tool_hierarchical(query, domain_hier, synthetic_tools_df, \"domain\")\n",
    "    domain_time = (time.time() - start) / 10\n",
    "    \n",
    "    scalability_results.append({\n",
    "        'tool_count': count,\n",
    "        'flat_time_ms': flat_time * 1000,\n",
    "        'hierarchical_time_ms': domain_time * 1000,\n",
    "        'speedup': flat_time / domain_time if domain_time > 0 else 0\n",
    "    })\n",
    "\n",
    "scalability_df = pd.DataFrame(scalability_results)\n",
    "print(\"Scalability Analysis:\")\n",
    "print(scalability_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scalability\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Execution time\n",
    "axes[0].plot(scalability_df['tool_count'], scalability_df['flat_time_ms'], \n",
    "             marker='o', label='Flat', linewidth=2)\n",
    "axes[0].plot(scalability_df['tool_count'], scalability_df['hierarchical_time_ms'], \n",
    "             marker='s', label='Hierarchical', linewidth=2)\n",
    "axes[0].set_title('Query Execution Time vs Number of Tools')\n",
    "axes[0].set_xlabel('Number of Tools')\n",
    "axes[0].set_ylabel('Time per Query (ms)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup\n",
    "axes[1].plot(scalability_df['tool_count'], scalability_df['speedup'], \n",
    "             marker='o', color='green', linewidth=2)\n",
    "axes[1].axhline(y=1, color='r', linestyle='--', label='No speedup')\n",
    "axes[1].set_title('Hierarchical Speedup vs Number of Tools')\n",
    "axes[1].set_xlabel('Number of Tools')\n",
    "axes[1].set_ylabel('Speedup Factor')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Hierarchy Design**: Both domain-based and action-based hierarchies can improve tool selection\n",
    "2. **Accuracy Trade-offs**: Hierarchical approaches may have different accuracy profiles depending on query types\n",
    "3. **Embedding Models**: Better embedding models improve accuracy but require more resources\n",
    "4. **Scalability**: Hierarchical approaches can significantly reduce search time for large tool sets\n",
    "5. **Hybrid Approaches**: Combining multiple signals (domain + action) can provide the best results\n",
    "\n",
    "### Next Steps:\n",
    "- Test with real LLM APIs (OpenAI, Anthropic) for actual tool calling\n",
    "- Implement dynamic hierarchy selection based on query analysis\n",
    "- Add more sophisticated error handling and fallback mechanisms\n",
    "- Explore learned embeddings specifically trained for tool selection\n",
    "- Consider context-aware approaches that learn from user patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
